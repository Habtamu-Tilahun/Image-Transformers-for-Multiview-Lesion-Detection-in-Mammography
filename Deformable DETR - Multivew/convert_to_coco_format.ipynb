{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Import some common libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "# Import omidb\n",
    "import omidb\n",
    "\n",
    "# Import detectron2, detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.structures import BoxMode\n",
    "import json\n",
    "from pycocotools import coco, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_omidb_dicts(csv_dir):\n",
    "    df = pd.read_csv(csv_dir)\n",
    "    \n",
    "    dataset_dicts = []\n",
    "    for idx, row in df.iterrows():\n",
    "        record = {}\n",
    "        filename = os.path.join(row[\"filename\"])\n",
    "\n",
    "        record[\"file_name\"] = filename\n",
    "        record[\"image_id\"] = idx\n",
    "        \n",
    "        if row['side'] == 'R':\n",
    "            # Bounding box breast area         \n",
    "            bbox = row[\"bbox\"][12:-1]\n",
    "            coords1 = bbox.split(',')\n",
    "            r= np.array([0,0,0,0])\n",
    "            indx1 = 0\n",
    "            for c in coords1:\n",
    "                aux = c.split('=')\n",
    "                r[indx1]=(int(aux[1]))\n",
    "                indx1 +=1\n",
    "\n",
    "            # we can get width and heigth from bbox\n",
    "            record[\"height\"] = r[3]-r[1]\n",
    "            record[\"width\"] = r[2]-r[0]\n",
    "            \n",
    "            # Bounding box roi  \n",
    "            bbox_roi = row[\"bbox_roi\"][12:-1]\n",
    "            coords2 = bbox_roi.split(',')\n",
    "            s= np.array([0,0,0,0])\n",
    "            indx2 = 0\n",
    "            for c in coords2:\n",
    "                aux = c.split('=')\n",
    "                s[indx2]=(int(aux[1]))\n",
    "                indx2 +=1\n",
    "            bbox_roi = omidb.mark.BoundingBox(s[0]-r[0],s[1]-r[1],s[2]-r[0],s[3]-r[1])\n",
    "            \n",
    "        else:\n",
    "            read_path = os.path.join(\"stacked_without_difference_image\", row[\"filename\"])\n",
    "            im = cv2.imread(read_path)\n",
    "            h,w,_ = im.shape\n",
    "            record[\"height\"] = h\n",
    "            record[\"width\"] = w\n",
    "\n",
    "            # Bounding box roi  \n",
    "            bbox_roi = row[\"transformed_bbox_roi\"][12:-1]\n",
    "            coords2 = bbox_roi.split(',')\n",
    "            s= np.array([0,0,0,0])\n",
    "            indx2 = 0\n",
    "            for c in coords2:\n",
    "                aux = c.split('=')\n",
    "                float_value = round(float(aux[1]), 0)\n",
    "                s[indx2]=(int(float_value))\n",
    "                indx2 +=1\n",
    "            bbox_roi = omidb.mark.BoundingBox(s[0],s[1],s[2],s[3])\n",
    "\n",
    "        px = [bbox_roi.x1, bbox_roi.x2, bbox_roi.x2, bbox_roi.x1]\n",
    "        py = [bbox_roi.y1, bbox_roi.y1, bbox_roi.y2, bbox_roi.y2]\n",
    "        poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
    "        poly = [p for x in poly for p in x]\n",
    "        objs = []\n",
    "        obj =  {\n",
    "                \"bbox\": [bbox_roi.x1 , bbox_roi.y1, bbox_roi.x2 - bbox_roi.x1, bbox_roi.y2 - bbox_roi.y1], #[x,y,w,h] format\n",
    "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "                \"segmentation\": [poly],\n",
    "                \"category_id\": 0,\n",
    "                }\n",
    "        objs.append(obj)\n",
    "        record[\"annotations\"] = objs\n",
    "        dataset_dicts.append(record)\n",
    "    return dataset_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NpEncoder, self).default(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output file path\n",
    "output_path = \"validation_annotations_multiview_without_diff.json\"\n",
    "\n",
    "# Load the dataset into a list of dictionaries\n",
    "dataset_dicts = get_omidb_dicts(\"/home/habtamu/Mammogram_Registration_Four_Resolution/transformed_lesion_validation_set.csv\")\n",
    "\n",
    "# Create a new COCO instance\n",
    "coco_data = coco.COCO()\n",
    "\n",
    "# Create a mapping from category names to category IDs\n",
    "category_map = {\"lesion\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the categories\n",
    "categories = [\n",
    "    {\"id\": 0, \"name\": \"lesion\"}\n",
    "]\n",
    "\n",
    "# Add the categories to the COCO dataset\n",
    "coco_data.dataset[\"categories\"] = categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the images and annotations lists in the COCO dataset\n",
    "coco_data.dataset[\"images\"] = []\n",
    "coco_data.dataset[\"annotations\"] = []\n",
    "# Add the images and annotations to the COCO dataset\n",
    "for image_dict in dataset_dicts:\n",
    "    image_id = image_dict[\"image_id\"]\n",
    "    image_width = image_dict[\"width\"]\n",
    "    image_width = int(image_width)\n",
    "    image_height = image_dict[\"height\"]\n",
    "    image_height = int(image_height)\n",
    "    image_file_name = image_dict[\"file_name\"]\n",
    "    coco_data.dataset[\"images\"].append({\n",
    "        \"id\": image_id,\n",
    "        \"width\": image_width,\n",
    "        \"height\": image_height,\n",
    "        \"file_name\": image_file_name\n",
    "    })\n",
    "\n",
    "    if \"annotations\" in image_dict:\n",
    "        for annotation_dict in image_dict[\"annotations\"]:\n",
    "            category_id = 0\n",
    "            bbox = annotation_dict[\"bbox\"]\n",
    "            segmentation = annotation_dict[\"segmentation\"]\n",
    "            segmentation_mask = mask.frPyObjects(segmentation, image_height, image_width)\n",
    "            area = int(mask.area(segmentation_mask)[0])\n",
    "            coco_data.dataset[\"annotations\"].append({\n",
    "                \"id\": len(coco_data.dataset[\"annotations\"]),\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": category_id,\n",
    "                \"bbox\": bbox,\n",
    "                \"area\": area,\n",
    "                \"segmentation\": segmentation,\n",
    "                \"iscrowd\": 0\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the COCO dataset to a JSON file\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(coco_data.dataset, f, cls=NpEncoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "habtamukernel",
   "language": "python",
   "name": "habtamukernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
